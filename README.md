# llm-pretrainer

A from-scratch LLM pretraining codebase focused on performance discipline â€” training a 360M parameter model with careful attention to throughput, memory efficiency, and MFU. Built with PyTorch and designed for single-node multi-GPU training.

Read the full write-up: [Training a 360M Parameter Model with Performance Discipline](https://www.saisasank.com/training-a-360m-parameter-model-with-performance-discipline/)
